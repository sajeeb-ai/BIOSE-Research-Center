{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163d270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 99 raw ECG numpy files with NaNs interpolated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt, resample_poly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Load CSV file ---\n",
    "df = pd.read_csv('combined_patient_ecg_outcome.csv')\n",
    "\n",
    "# --- Step 2: Parsing function for space-separated ECG string to 1D numpy array ---\n",
    "\n",
    "def string_to_float_array_fixed(s):\n",
    "    values = []\n",
    "    for val in s.strip().split():\n",
    "        try:\n",
    "            values.append(float(val))\n",
    "        except:\n",
    "            values.append(np.nan)\n",
    "    return np.array(values)\n",
    "\n",
    "df['data_array'] = df['data'].apply(string_to_float_array_fixed)\n",
    "\n",
    "# --- Step 3: NaN interpolation helper ---\n",
    "\n",
    "def interpolate_nan(data):\n",
    "    n = len(data)\n",
    "    if n == 0:\n",
    "        return data\n",
    "\n",
    "    indices = np.arange(n)\n",
    "    valid = ~np.isnan(data)\n",
    "\n",
    "    if valid.sum() == 0:\n",
    "        return np.zeros_like(data)\n",
    "\n",
    "    return np.interp(indices, indices[valid], data[valid])\n",
    "\n",
    "# --- Step 4: Save raw numpy arrays with NaNs interpolated ---\n",
    "\n",
    "np_folder = 'ecg_numpy_arrays'\n",
    "os.makedirs(np_folder, exist_ok=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    patient_id = row['patient_id']\n",
    "    ecg_array = row['data_array']\n",
    "    ecg_array_clean = interpolate_nan(ecg_array)\n",
    "    np.save(os.path.join(np_folder, f\"{patient_id}_ecg.npy\"), ecg_array_clean)\n",
    "\n",
    "print(f\"Saved {len(df)} raw ECG numpy files with NaNs interpolated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa7cb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>data_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>284</td>\n",
       "      <td>24433.0 24317.0 24354.0 24377.0 24270.0 24272....</td>\n",
       "      <td>1</td>\n",
       "      <td>[24433.0, 24317.0, 24354.0, 24377.0, 24270.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>286</td>\n",
       "      <td>-31920.0 -31920.0 -31920.0 -31920.0 -31920.0 -...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-31920.0, -31920.0, -31920.0, -31920.0, -3192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296</td>\n",
       "      <td>3221.0 3274.0 3380.0 3098.0 2843.0 2910.0 3274...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3221.0, 3274.0, 3380.0, 3098.0, 2843.0, 2910....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299</td>\n",
       "      <td>18584.0 18488.0 18648.0 19008.0 19239.0 19347....</td>\n",
       "      <td>1</td>\n",
       "      <td>[18584.0, 18488.0, 18648.0, 19008.0, 19239.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303</td>\n",
       "      <td>26985.0 10038.0 -9400.0 -20411.0 -18704.0 -419...</td>\n",
       "      <td>1</td>\n",
       "      <td>[26985.0, 10038.0, -9400.0, -20411.0, -18704.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                                               data  label  \\\n",
       "0         284  24433.0 24317.0 24354.0 24377.0 24270.0 24272....      1   \n",
       "1         286  -31920.0 -31920.0 -31920.0 -31920.0 -31920.0 -...      1   \n",
       "2         296  3221.0 3274.0 3380.0 3098.0 2843.0 2910.0 3274...      1   \n",
       "3         299  18584.0 18488.0 18648.0 19008.0 19239.0 19347....      1   \n",
       "4         303  26985.0 10038.0 -9400.0 -20411.0 -18704.0 -419...      1   \n",
       "\n",
       "                                          data_array  \n",
       "0  [24433.0, 24317.0, 24354.0, 24377.0, 24270.0, ...  \n",
       "1  [-31920.0, -31920.0, -31920.0, -31920.0, -3192...  \n",
       "2  [3221.0, 3274.0, 3380.0, 3098.0, 2843.0, 2910....  \n",
       "3  [18584.0, 18488.0, 18648.0, 19008.0, 19239.0, ...  \n",
       "4  [26985.0, 10038.0, -9400.0, -20411.0, -18704.0...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0494f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, resample_poly, medfilt\n",
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010745e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Butterworth low-pass filter and helpers\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=4):\n",
    "    from scipy.signal import butter\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=3):\n",
    "    from scipy.signal import filtfilt\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    data = data[:, np.newaxis] if data.ndim == 1 else data\n",
    "    filtered = filtfilt(b, a, data, axis=0)\n",
    "    return filtered.squeeze()\n",
    "\n",
    "def resample_ecg(data, original_fs, target_fs):\n",
    "    from scipy.signal import resample_poly\n",
    "    from math import gcd\n",
    "    g = gcd(int(original_fs), int(target_fs))\n",
    "    up = int(target_fs // g)\n",
    "    down = int(original_fs // g)\n",
    "    data = data[:, np.newaxis] if data.ndim == 1 else data\n",
    "    resampled = resample_poly(data, up, down, axis=0)\n",
    "    return resampled.squeeze()\n",
    "\n",
    "def median_filter_denoise(signal, kernel_size=5):\n",
    "    return medfilt(signal, kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd10871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ECG signal processing (filter, resample, median filter)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:06<00:00, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Saved filtered ECG files to 'ecg_resampled_filtered_arrays_order3'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process raw ECG numpy files: filtering, resampling, median filtering\n",
    "\n",
    "original_fs = 310.0\n",
    "target_fs = 45.0\n",
    "cutoff_frequency = 20.0\n",
    "filter_order = 3\n",
    "median_kernel_size = 5\n",
    "\n",
    "filtered_folder = 'ecg_resampled_filtered_arrays_order3'\n",
    "os.makedirs(filtered_folder, exist_ok=True)\n",
    "\n",
    "print(\"Starting ECG signal processing (filter, resample, median filter)...\")\n",
    "for filename in tqdm(os.listdir(np_folder)):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(np_folder, filename)\n",
    "        ecg_data = np.load(file_path)\n",
    "\n",
    "        # Resample\n",
    "        ecg_resampled = resample_ecg(ecg_data, original_fs, target_fs)\n",
    "\n",
    "        # Median filter to remove spikes\n",
    "        ecg_median = median_filter_denoise(ecg_resampled, kernel_size=median_kernel_size)\n",
    "\n",
    "        # Butterworth low-pass filter to smooth signal\n",
    "        ecg_filtered = butter_lowpass_filter(ecg_median, cutoff_frequency, target_fs, filter_order)\n",
    "\n",
    "        # Save filtered ECG\n",
    "        save_path = os.path.join(filtered_folder, filename.replace('_ecg.npy', '_processed.npy'))\n",
    "        np.save(save_path, ecg_filtered)\n",
    "\n",
    "print(f\"Processing completed. Saved filtered ECG files to '{filtered_folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bb44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class to load processed raw ECG and segment into windows\n",
    "\n",
    "class RawECGSegmentDataset(Dataset):\n",
    "    def __init__(self, np_folder, labels_dict, fs=45, window_sec=60, overlap_sec=10):\n",
    "        self.fs = fs\n",
    "        self.window_sec = window_sec\n",
    "        self.overlap_sec = overlap_sec\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "\n",
    "        for file in tqdm(os.listdir(np_folder), desc=\"Loading processed ECG files\"):\n",
    "            if file.endswith('.npy'):\n",
    "                pid = int(file.split('_')[0])\n",
    "                ecg = np.load(os.path.join(np_folder, file))\n",
    "\n",
    "                window_len = int(window_sec * fs)\n",
    "                overlap_len = int(overlap_sec * fs)\n",
    "                step = window_len - overlap_len\n",
    "\n",
    "                for start in range(0, len(ecg) - window_len + 1, step):\n",
    "                    segment = ecg[start:start+window_len]\n",
    "                    self.samples.append(segment)\n",
    "                    self.labels.append(labels_dict[pid])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.samples[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa4b6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading processed ECG files: 100%|██████████| 198/198 [00:01<00:00, 103.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments in dataset: 14006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate dataset and split into train/val\n",
    "\n",
    "labels_dict = dict(zip(df['patient_id'].astype(int), df['label']))\n",
    "raw_dataset = RawECGSegmentDataset(np_folder=filtered_folder, labels_dict=labels_dict,\n",
    "                                  fs=target_fs, window_sec=60, overlap_sec=10)\n",
    "\n",
    "print(f\"Total segments in dataset: {len(raw_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808df0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajee\\AppData\\Local\\Temp\\ipykernel_19124\\487114622.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  self.signals = torch.tensor(signals, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels arrays to split\n",
    "features = [raw_dataset[i][0].numpy() for i in range(len(raw_dataset))]\n",
    "labels = [raw_dataset[i][1].item() for i in range(len(raw_dataset))]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=labels)\n",
    "\n",
    "class ECGTensorDataset(Dataset):\n",
    "    def __init__(self, signals, labels):\n",
    "        self.signals = torch.tensor(signals, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.signals[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ECGTensorDataset(X_train, y_train)\n",
    "val_dataset = ECGTensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3a06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9: Define 1D CNN + Attention model ---\n",
    "\n",
    "class RawECGAttention1DCNN(nn.Module):\n",
    "    def __init__(self, input_len, num_classes=2, dropout=0.3):\n",
    "        super(RawECGAttention1DCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(128, 1, kernel_size=1),\n",
    "            nn.Softmax(dim=2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        attn_weights = self.attention(x)\n",
    "        x = (x * attn_weights).sum(dim=2)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c30cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Training: 100%|██████████| 351/351 [02:48<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.6689, Val Accuracy=0.6463\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.6446, Val Accuracy=0.6574\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.6170, Val Accuracy=0.6820\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.5940, Val Accuracy=0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 Training: 100%|██████████| 351/351 [02:36<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.5776, Val Accuracy=0.7152\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 Training: 100%|██████████| 351/351 [02:37<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.5800, Val Accuracy=0.7320\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 Training: 100%|██████████| 351/351 [02:44<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.5352, Val Accuracy=0.6842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 Training: 100%|██████████| 351/351 [02:33<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.5429, Val Accuracy=0.6788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 Training: 100%|██████████| 351/351 [02:33<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.5157, Val Accuracy=0.7641\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.5017, Val Accuracy=0.7452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.4828, Val Accuracy=0.7762\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Training: 100%|██████████| 351/351 [02:37<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.4712, Val Accuracy=0.7937\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 Training: 100%|██████████| 351/351 [02:38<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.4724, Val Accuracy=0.7741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 Training: 100%|██████████| 351/351 [02:37<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.4450, Val Accuracy=0.7859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 Training: 100%|██████████| 351/351 [02:36<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=0.4407, Val Accuracy=0.7855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 Training: 100%|██████████| 351/351 [02:39<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.4414, Val Accuracy=0.7919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 Training: 100%|██████████| 351/351 [02:42<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=0.4181, Val Accuracy=0.8048\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 Training: 100%|██████████| 351/351 [02:37<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=0.4374, Val Accuracy=0.8012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 Training: 100%|██████████| 351/351 [02:37<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss=0.4006, Val Accuracy=0.8123\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 Training: 100%|██████████| 351/351 [02:40<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=0.3885, Val Accuracy=0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 Training: 100%|██████████| 351/351 [02:34<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss=0.3997, Val Accuracy=0.8169\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 Training: 100%|██████████| 351/351 [02:38<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss=0.3758, Val Accuracy=0.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 Training: 100%|██████████| 351/351 [02:39<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss=0.3623, Val Accuracy=0.8448\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 Training: 100%|██████████| 351/351 [02:44<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss=0.3667, Val Accuracy=0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss=0.3477, Val Accuracy=0.8476\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 Training: 100%|██████████| 351/351 [02:36<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss=0.3358, Val Accuracy=0.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 Training: 100%|██████████| 351/351 [03:02<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss=0.3395, Val Accuracy=0.8480\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 Training: 100%|██████████| 351/351 [02:59<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss=0.3281, Val Accuracy=0.8587\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 Training: 100%|██████████| 351/351 [02:51<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss=0.3235, Val Accuracy=0.8469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss=0.3018, Val Accuracy=0.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 Training: 100%|██████████| 351/351 [02:38<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss=0.2891, Val Accuracy=0.8947\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 Training: 100%|██████████| 351/351 [02:54<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss=0.2819, Val Accuracy=0.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 Training: 100%|██████████| 351/351 [02:36<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss=0.2827, Val Accuracy=0.8851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 Training: 100%|██████████| 351/351 [02:36<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss=0.2755, Val Accuracy=0.8801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss=0.2706, Val Accuracy=0.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 Training: 100%|██████████| 351/351 [02:39<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss=0.2721, Val Accuracy=0.8997\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 Training: 100%|██████████| 351/351 [02:38<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss=0.2432, Val Accuracy=0.8665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 Training: 100%|██████████| 351/351 [02:37<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss=0.2514, Val Accuracy=0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 Training: 100%|██████████| 351/351 [02:39<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss=0.2521, Val Accuracy=0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 Training: 100%|██████████| 351/351 [02:33<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss=0.2393, Val Accuracy=0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 Training: 100%|██████████| 351/351 [02:32<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss=0.2330, Val Accuracy=0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 Training: 100%|██████████| 351/351 [02:34<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss=0.2310, Val Accuracy=0.9040\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss=0.2314, Val Accuracy=0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 Training: 100%|██████████| 351/351 [02:36<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss=0.2705, Val Accuracy=0.9065\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss=0.2258, Val Accuracy=0.8412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 Training: 100%|██████████| 351/351 [02:35<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss=0.2185, Val Accuracy=0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 Training: 100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss=0.2177, Val Accuracy=0.9126\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 Training: 100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss=0.2219, Val Accuracy=0.9026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 Training: 100%|██████████| 351/351 [02:29<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss=0.2112, Val Accuracy=0.9108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 Training: 100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss=0.2231, Val Accuracy=0.9201\n",
      "Saved best model\n"
     ]
    }
   ],
   "source": [
    "# Train and validate the model with AdamW and early stopping\n",
    "\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "input_len = X_train[0].shape[0]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "model = RawECGAttention1DCNN(input_len=input_len, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 7\n",
    "best_val_acc = 0.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Accuracy={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Saved best model\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d885b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,5))\n",
    "\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
    "# plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Validation Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5ecfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Poor       0.93      0.91      0.92      1442\n",
      "        Good       0.91      0.93      0.92      1360\n",
      "\n",
      "    accuracy                           0.92      2802\n",
      "   macro avg       0.92      0.92      0.92      2802\n",
      "weighted avg       0.92      0.92      0.92      2802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make sure your model and device are defined as in training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "class_names = ['Poor', 'Good']  # Update as per your classes\n",
    "\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, digits=2)\n",
    "print(\"Detailed Classification Report on Validation Set:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd7db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report on validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Poor     0.9307    0.9126    0.9216      1442\n",
      "        Good     0.9092    0.9279    0.9185      1360\n",
      "\n",
      "    accuracy                         0.9201      2802\n",
      "   macro avg     0.9200    0.9203    0.9200      2802\n",
      "weighted avg     0.9203    0.9201    0.9201      2802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model and print classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# After model evaluation and collecting all_preds and all_labels\n",
    "\n",
    "class_names = ['Poor', 'Good']  # Replace with your actual class names\n",
    "\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
    "print(\"Detailed classification report on validation set:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 1–4: Your original code to load and save raw ECG arrays.\n",
    "\n",
    "# Steps 5–6: Filter, resample, median-filter, and save processed ECG arrays.\n",
    "\n",
    "# Step 7: New Dataset that loads these processed ECG signals and segments into smaller windows.\n",
    "\n",
    "# Step 8: Split dataset and create PyTorch DataLoader.\n",
    "\n",
    "# Step 9: Define the 1D CNN + attention model that takes raw ECG windows.\n",
    "\n",
    "# Step 10: Train with AdamW optimizer, CrossEntropyLoss, early stopping.\n",
    "\n",
    "# Step 11: Evaluate best model and print detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Takes a raw ECG signal (a sequence of numbers representing heart activity over time).\n",
    "\n",
    "# Pre-process: median and butterworth-low pass filter\n",
    "\n",
    "# CNN Layers:\n",
    "\n",
    "# The input raw ECG window (a sequence of voltage values over time) passes through several 1D convolutional layers.\n",
    "\n",
    "# Each convolution layer extracts increasingly complex features:\n",
    "\n",
    "# Early layers might detect simple patterns like edges or peaks (e.g., QRS complexes).\n",
    "\n",
    "# Deeper layers combine those to capture rhythms, morphology, and higher-level ECG patterns.\n",
    "\n",
    "# These layers output a feature map — a set of learned features across time and channels.\n",
    "\n",
    "# Attention Layer:\n",
    "\n",
    "# The attention mechanism takes these CNN feature maps as input.\n",
    "\n",
    "# It assigns weights (importance scores) to each time step or segment of the feature map.\n",
    "\n",
    "# This means it highlights which parts of the signal are most relevant for classification.\n",
    "\n",
    "# It produces a weighted summary (a context vector), emphasizing important time regions dynamically.\n",
    "\n",
    "# Classification: Passes the summarized features through two dense layers to predict the class (e.g., normal or abnormal heart activity).\n",
    "\n",
    "# Regularization: Uses dropout to prevent overfitting and batch normalization to stabilize training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
